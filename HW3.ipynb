{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุชูุฑู ุณูู<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Word2Vec & MLP</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ุนู ูุฑุชูุช</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">ali.fartout@ut.ac.ir</p>\n",
    "\n",
    "    \n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ุนูุฑุถุง ุขุฎููุฏ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">a.akhoundi79@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">๐ ูุดุฎุตุงุช ุฏุงูุดุฌู:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ูุงู ู ูุงู ุฎุงููุงุฏฺฏ: {{ูพูุง ุฌูู ุฏู}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ุดูุงุฑู ุฏุงูุดุฌู: {{810102106}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ุชุงุฑุฎ ุงุฑุณุงู: {{ุชุงุฑุฎ_ุงุฑุณุงู}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "๐ <b>ุณุงุฎุชุงุฑ ุชูุฑู:</b>\n",
    "<li><b>ุณูุงู ุงูู - <span dir=\"rtl\">ุณูุงู ุงูู : ูพุงุฏู ุณุงุฒ CBOW ู Skip-Gram</span> </b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏู </li>\n",
    "<li>ุจุฎุด ุฏูู: ูพุด ูพุฑุฏุงุฒุด </li>\n",
    "<li>ุจุฎุด ุณูู: ูพุงุฏู ุณุงุฒ ุดุจฺฉู ูโ ุขููุฒุด ุดุจฺฉู</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู:ููุงุณู ูุฏูโูุง</li>\n",
    "</ul>\n",
    "<li><b>ุณูุงู ุฏูู - <span dir=\"rtl\"> ูพุงุฏูโุณุงุฒ ุทุจููโุจูุฏ ุงุฎุจุงุฑ ุจุง ฺฉูฺฉ ุดุจฺฉู ุนุตุจ ู ูุฏู Fasttext</span> </b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏู ู ูพุด ูพุฑุฏุงุฒุด </li>\n",
    "<li>ุจุฎุด ุฏูู: Embedding ูููููโูุง </li>\n",
    "<li>ุจุฎุด ุณูู: ูพุงุฏู ุณุงุฒ ุดุจฺฉู ูโ ุงููุฒุด ุดุจฺฉู</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู: ุชุญูู ูุชุงุฌ</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "๐ก <b>ูฺฉุงุช ููู:.</b>\n",
    "<br>\n",
    " ๐กุจุฑุง ุณูุงู ุงูู ุจุงุฏ ุชูุงู ุจุฎุด ูุง ุฑู ุฎูุฏุชุงู ูพุงุฏู ุณุงุฒ ฺฉูุฏ . ุญู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุงูุงุฏู ุฑุง ูุฏุงุฑุฏ.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุงูู : ูพุงุฏู ุณุงุฒ CBOW ู Skip-Gram<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงู ุดูุง ุจุง ูพุงุฏู ุณุงุฒ ุฏู ูุฏู CBOW ู Skipgram ุขุดูุง ูุดูุฏ.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏู</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "\n",
    "ุงุจุชุฏุง ุฏุชุงุณุช ุฒุฑ ุฑุง ุฏุงูููุฏ ฺฉูุฏ.\n",
    "<a>https://docs.pytorch.org/text/0.8.1/datasets.html#wikitext-2</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ณ ูุงู txt ฺฉู ุจุฑุง ุณู ูุฌููุนู ุฏุงุฏู Train, Valid, Test\n",
    "ุฐุฎุฑู ุดุฏู ุจุงุดุฏ. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/npl_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = dataset['train']\n",
    "valid = dataset['validation']\n",
    "test = dataset['test']\n",
    "\n",
    "\n",
    "with open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in test:\n",
    "        f.write(line['text'] + \"\\n\")\n",
    "      \n",
    "with open(\"valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in valid:\n",
    "        f.write(line['text'] + \"\\n\")\n",
    "                \n",
    "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in train:\n",
    "        f.write(line['text'] + \"\\n\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุด ูพุฑุฏุงุฒุด</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "ุจุนุฏ ุงุฒ ุงูฺฉู ุฏุงุฏู ุฑุง ุจุงุฑฺฏุฐุงุฑ ฺฉุฑุฏุฏ. ุชูุงู ูุฑุงุญู ุฒุฑ ุฑุง ุฎูุฏุชุงู ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู ู ูพุด ูพุฑุฏุงุฒุด ูุง ฺฏูุชู ุดุฏู ุฑุง ุงูุฌุงู ุฏูุฏ\n",
    "<br>\n",
    "\n",
    "ุจุฎุด ุงูู) ฺฉ ุชุงุจุน ฺฉู ูุฑุงุญู ุฒุฑ ุฑุง ุงูุฌุงู ุฏูุฏ:\n",
    "โ<br>\n",
    "โฑ) ุชูุงู ฺฉููุงุช ุฑุง lowercase ุดูุฏ\n",
    "โ<br>\n",
    "ฒ)Special Characters ุญุฐู ุดููุฏ\n",
    "โ<br>\n",
    "ณ)ฺฉููุงุช ุฏุฑ ูุฑ space (ูุงุตูู) ุงุฒ ูู ุฌุฏุง ู ุชูฺฉู ุดููุฏ\n",
    "โ<br>\n",
    "\n",
    "\n",
    "ุจุฎุด ุฏูู)  \n",
    "ฑ)ุดูุงุฑุด ูุฑฺฉุงูุณ (Frequency Counting): ุชุนุฏุงุฏ ุชฺฉุฑุงุฑ ูุฑ ฺฉููู ุฏุฑ ุชูุงู ูุชูโูุง ุฑุง ุญุณุงุจ ฺฉูุฏ\n",
    "โ<br>\n",
    "ฒ)ููุชุฑ ฺฉุฑุฏู ฺฉููุงุช ูุงุฏุฑ (Min Frequency Filtering): ููุท ฺฉููุงุช ฺฉู ุจุดุชุฑ ุงุฒ min_freq ุจุงุฑ ุชฺฉุฑุงุฑ ุดุฏูโุงูุฏ ุฑุง ูฺฏู ุฏุงุฑุฏ\n",
    "โ<br>\n",
    "ณ)ุงุถุงูู ฺฉุฑุฏู ุชูฺฉู ุฎุงุต <unk>: ุจุฑุง ฺฉููุงุช ูุงุดูุงุฎุชู ุง ูุงุฏุฑ\n",
    "โ<br>\n",
    "ด)ุงุฌุงุฏ ุฏฺฉุดูุฑ ุฏูุทุฑูู:\n",
    "โ<br>\n",
    "word โ index (string to index)\n",
    "โ<br>\n",
    "index โ word (index to string)\n",
    "\n",
    "\n",
    "ุจุฎุด ุณูู)\n",
    "<br>\n",
    "ฑ) ุชุงุจุน ุจููุณุฏ ฺฉู ุงุฒ ฺฉ ุฌูููุ ูููููโูุง ุขููุฒุด CBOW ุชููุฏ ฺฉูุฏ\n",
    "<br>\n",
    "ฒ)ุชุงุจุน ุจููุณุฏ ฺฉู ุงุฒ ฺฉ ุฌูููุ ูููููโูุง ุขููุฒุด Skip-gram ุชููุฏ ฺฉูุฏ\n",
    "<br>\n",
    "<b>ุจุฎุด ุณูู ุฑุง ุชูุถุญ ุจุฏูุฏ</b>\n",
    "\n",
    "<b>ูฺฉุชู: ุจุฑุง ุฎูุงูุง ุฏุฑ ฺฉุฏ ุจู ุฌุง ูพุงุฏู ุณุงุฒ ูุนููู ุฏู ุชุงุจุนุ  ูุชูุงูุฏ ุฏู ุชุงุจุน ฺฏูุชู ุดุฏู ุจุงูุง ุฑุง ุจุฑุง collate_fn ูพุงุชูุฑฺ ูพุงุฏูโุณุงุฒ ฺฉูุฏ.</b>\n",
    "\n",
    "<a>https://discuss.pytorch.org/t/custom-collate-function/145823</a>\n",
    "<br>\n",
    "ุจุฎุด ฺูุงุฑู)\n",
    "<br>\n",
    "ุฏุฑ ุขุฎุฑ ุฏุงุฏู ูพุฑุฏุงุฒุด ุดุฏู ุฑุง ุฏุฑ dataloader ููุฏ ฺฉูุฏ.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ุจู ุนููุงู ูุซุงู\n",
    "<br>\n",
    "ุจุฑุง ุจุฎุด ุงูู \n",
    "\"Hello World! This is a TEST sentence, with 123 numbers.\"\n",
    "ุจู ุนูุงู ูุฑูุฏ\n",
    "[\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\", \"with\", \"123\", \"numbers\"]\n",
    "ุดูุฏ\n",
    "\n",
    "<br>\n",
    "ุจุฑุง ุจุฎุด ุฏูู\n",
    "<br>\n",
    "ูุฑูุฏ:\n",
    "texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "min_freq = 2\n",
    "<br>\n",
    "ุฎุฑูุฌ:\n",
    "<br>\n",
    "Vocabulary:\n",
    "{\n",
    "    \"<unk>\": 0,\n",
    "    \"the\": 1,\n",
    "    \"sat\": 2,\n",
    "    \"on\": 3,\n",
    "    \"cat\": 4, \n",
    "    \"dog\": 5\n",
    "}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ:</b><br>\n",
    "{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "\n",
    "with open(\"test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        test.append(line)\n",
    "\n",
    "with open(\"valid.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        valid.append(line)\n",
    "\n",
    "with open(\"train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        train.append(line)\n",
    "\n",
    "\n",
    "class Cbow(Dataset):\n",
    "    def __init__(self,texts,min_freq=2,windows_size=2):\n",
    "        self.min_freq = min_freq\n",
    "        self.windows_size = windows_size\n",
    "        self.texts = texts\n",
    "        self.words_list , self.cleaned_text= self.__cleaned()\n",
    "        self.word2idx , self.idx2word  = self.__Frequency_Counting()\n",
    "        self.data = self._build_cbow_pairs()\n",
    "            \n",
    "    def __cleaned(self,):\n",
    "        cleaned_text = []\n",
    "        words_list = []\n",
    "        pattern = re.compile(r'[^A-Za-z0-9\\s]')\n",
    "        for text in self.texts:\n",
    "            if text !='':\n",
    "                clean_text = pattern.sub('', text).lower()\n",
    "                words = [word for word in clean_text.split() if word]\n",
    "                words_list.extend(words)\n",
    "                cleaned_text.append(words)\n",
    "        self.words_list = words_list\n",
    "        self.cleaned_text = cleaned_text     \n",
    "        return self.words_list,self.cleaned_text\n",
    "    \n",
    "    def __Frequency_Counting(self):\n",
    "        word_counts = {}\n",
    "        for text in self.words_list:\n",
    "            if text in word_counts.keys():\n",
    "                word_counts[text] = int(word_counts[text]) + 1            \n",
    "            else :\n",
    "                word_counts[text] = 1\n",
    "        list_word = [w for w,c in word_counts.items() if c>=self.min_freq]\n",
    "        list_word = list_word + [\"<unk>\"]\n",
    "        self.word2idx = {word:idx for idx,word in enumerate(list_word)}\n",
    "        self.idx2word = {idx:word for word,idx in self.word2idx.items()}\n",
    "        return self.word2idx,self.idx2word    \n",
    "    \n",
    "    def _build_cbow_pairs(self):\n",
    "        dataset = []\n",
    "        for sentence in self.cleaned_text:\n",
    "            for i, target in enumerate(sentence):\n",
    "                context = []\n",
    "                for j in range(-self.windows_size, self.windows_size + 1):\n",
    "                    if j == 0: \n",
    "                        continue\n",
    "                    if 0 <= i + j < len(sentence):\n",
    "                        context.append(sentence[i + j])\n",
    "\n",
    "                if len(context) == 0:\n",
    "                    continue\n",
    "                \n",
    "                context_ids = [self.word2idx.get(w, 1) for w in context]\n",
    "                target_id  = self.word2idx.get(target, 1)\n",
    "\n",
    "                dataset.append((torch.tensor(context_ids, dtype=torch.long), target_id))#append((context_ids, target_id))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def cbow_collate_fn(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "\n",
    "    padded_contexts = torch.nn.utils.rnn.pad_sequence(\n",
    "        contexts, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    targets = torch.tensor(targets)\n",
    "\n",
    "    return padded_contexts, targets        \n",
    "        \n",
    "class Skipgram(Dataset):\n",
    "    def __init__(self,texts,min_freq=2,windows_size=2):\n",
    "        self.min_freq = min_freq\n",
    "        self.windows_size = windows_size\n",
    "        self.texts = texts\n",
    "        self.words_list , self.cleaned_text= self.__cleaned()\n",
    "        self.word2idx , self.idx2word  = self.__Frequency_Counting()\n",
    "        self.data = self._build_skipgram_pairs()\n",
    "            \n",
    "    def __cleaned(self):\n",
    "        cleaned_text = []\n",
    "        words_list = []\n",
    "        pattern = re.compile(r'[^A-Za-z0-9\\s]')\n",
    "        for text in self.texts:\n",
    "            if text !='':\n",
    "                clean_text = pattern.sub('', text).lower()\n",
    "                words = [word for word in clean_text.split() if word]\n",
    "                words_list.extend(words)\n",
    "                cleaned_text.append(words)\n",
    "        self.words_list = words_list\n",
    "        self.cleaned_text = cleaned_text     \n",
    "        return self.words_list,self.cleaned_text\n",
    "    \n",
    "    def __Frequency_Counting(self):\n",
    "        word_counts = {}\n",
    "        for text in self.words_list:\n",
    "            if text in word_counts.keys():\n",
    "                word_counts[text] = int(word_counts[text]) + 1            \n",
    "            else :\n",
    "                word_counts[text] = 1\n",
    "        list_word = [w for w,c in word_counts.items() if c>=self.min_freq]\n",
    "        list_word = list_word + [\"<unk>\"]\n",
    "        self.word2idx = {word:idx for idx,word in enumerate(list_word)}\n",
    "        self.idx2word = {idx:word for word,idx in self.word2idx.items()}\n",
    "        return self.word2idx,self.idx2word    \n",
    "    \n",
    "    def _build_skipgram_pairs(self):\n",
    "        dataset = []\n",
    "        for sentence in self.cleaned_text:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                target_id = self.word2idx.get(target_word, 1)\n",
    "                for j in range(-self.windows_size, self.windows_size + 1):\n",
    "                    if j == 0:\n",
    "                        continue\n",
    "                    if 0 <= i + j < len(sentence):\n",
    "                        context_word = sentence[i + j]\n",
    "                        context_id = self.word2idx.get(context_word, 1)\n",
    "                        dataset.append((torch.tensor(context_id, dtype=torch.long), target_id))#append((target_id, context_id))\n",
    "        return dataset\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def skipgram_collate_fn(batch):\n",
    "    targets = torch.tensor([x[0] for x in batch])\n",
    "    contexts = torch.tensor([x[1] for x in batch])\n",
    "    return targets, contexts   \n",
    "\n",
    "batch_size = 512\n",
    "windows_size = 3\n",
    "min_freq = 3\n",
    "\n",
    "train_dataset_cbow = Cbow(train, windows_size=windows_size, min_freq=min_freq)\n",
    "valid_dataset_cbow = Cbow(valid, windows_size=windows_size, min_freq=min_freq)\n",
    "test_dataset_cbow  = Cbow(test, windows_size=windows_size,min_freq=min_freq)\n",
    "\n",
    "train_dataset_skipgram = Skipgram(train, windows_size=windows_size, min_freq=min_freq)\n",
    "valid_dataset_skipgram = Skipgram(valid, windows_size=windows_size, min_freq=min_freq)\n",
    "test_dataset_skipgram  = Skipgram(test, windows_size=windows_size, min_freq=min_freq)\n",
    "\n",
    "train_loader_cbow = DataLoader(train_dataset_cbow, batch_size=batch_size, shuffle=True, collate_fn=cbow_collate_fn)\n",
    "valid_loader_cbow = DataLoader(valid_dataset_cbow, batch_size=batch_size, collate_fn=cbow_collate_fn)\n",
    "test_loader_cbow = DataLoader(test_dataset_cbow, batch_size=batch_size, collate_fn=cbow_collate_fn)\n",
    "\n",
    "train_loader_skipgram = DataLoader(train_dataset_skipgram, batch_size=batch_size, shuffle=True, collate_fn=skipgram_collate_fn)\n",
    "valid_loader_skipgram = DataLoader(valid_dataset_skipgram, batch_size=batch_size ,collate_fn=skipgram_collate_fn)\n",
    "test_loader_skipgram = DataLoader(test_dataset_skipgram, batch_size=batch_size ,collate_fn=skipgram_collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุงุฏูโุณุงุฒ ุดุจฺฉู ู ุขููุฒุด ุดุจฺฉู</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "ุญุงู ุดุจฺฉูโูุง SkipGram ู CBOW ุฑุง ูุงููุฏ ููุงูู (ุง ฺฉุชุงุจ ุฏุฑุณ) ูพุงุฏู ุณุงุฒ ฺฉูุฏ </p>\n",
    "ุดุจฺฉู ุฑุง ุขููุฒุด ุฏูุฏ ู ุจุฑุง ุฏุงุฏฺฏุงู Train, Validation & Test ูููุฏุงุฑ ุฎุทุง ุชุฑุณู ฺฉูุฏ.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "38911\n",
      "Training CBOW...\n",
      "CBOW epoch 01 train_loss=7.5875 test_loss=11.3923 test_acc=0.0130\n",
      "CBOW epoch 02 train_loss=7.4864 test_loss=11.4687 test_acc=0.0178\n",
      "CBOW epoch 03 train_loss=7.4733 test_loss=11.6346 test_acc=0.0119\n",
      "CBOW epoch 04 train_loss=7.4709 test_loss=11.6519 test_acc=0.0110\n",
      "CBOW epoch 05 train_loss=7.4693 test_loss=11.6443 test_acc=0.0101\n",
      "CBOW epoch 06 train_loss=7.4696 test_loss=11.7662 test_acc=0.0120\n",
      "CBOW epoch 07 train_loss=7.4715 test_loss=11.8195 test_acc=0.0111\n",
      "CBOW epoch 08 train_loss=7.4699 test_loss=11.6908 test_acc=0.0120\n",
      "CBOW epoch 09 train_loss=7.4688 test_loss=11.7873 test_acc=0.0123\n",
      "CBOW epoch 10 train_loss=7.4695 test_loss=11.8850 test_acc=0.0125\n",
      "Training Skip-gram...\n",
      "Skip epoch 01 train_loss=9.1953 test_loss=14.2682 test_acc=0.0142\n",
      "Skip epoch 02 train_loss=9.3348 test_loss=14.1357 test_acc=0.0129\n",
      "Skip epoch 03 train_loss=9.3427 test_loss=14.2468 test_acc=0.0136\n",
      "Skip epoch 04 train_loss=9.3452 test_loss=14.2791 test_acc=0.0133\n",
      "Skip epoch 05 train_loss=9.3447 test_loss=14.0920 test_acc=0.0132\n",
      "Skip epoch 06 train_loss=9.3444 test_loss=14.1732 test_acc=0.0120\n",
      "Skip epoch 07 train_loss=9.3459 test_loss=14.1853 test_acc=0.0136\n",
      "Skip epoch 08 train_loss=9.3467 test_loss=14.2399 test_acc=0.0126\n",
      "Skip epoch 09 train_loss=9.3466 test_loss=13.8119 test_acc=0.0126\n",
      "Skip epoch 10 train_loss=9.3461 test_loss=14.0060 test_acc=0.0134\n",
      "\n",
      "Example neighbors using CBOW embeddings:\n",
      " i -> [('oranges', 0.7824485898017883), ('learned', 0.7812898755073547), ('hooper', 0.7563483119010925), ('placenames', 0.7547097206115723)]\n",
      " deep -> [('kotagiri', 0.763290286064148), ('constructed', 0.7573050260543823), ('ring', 0.7494611740112305), ('revenues', 0.7452742457389832)]\n",
      " python -> [('quince', 0.7938624620437622), ('neighborhoods', 0.7759628295898438), ('colborne', 0.7666295766830444), ('mania', 0.765206515789032)]\n",
      " learning -> [('step', 0.7443315386772156), ('venkatesh', 0.7267026305198669), ('saccardo', 0.696527898311615), ('stalked', 0.6886771321296692)]\n",
      "\n",
      "Example neighbors using Skip-gram embeddings:\n",
      " i -> [('strain', 0.8644610047340393), ('jaws', 0.8116984963417053), ('renovations', 0.7960455417633057), ('displeased', 0.7943649888038635)]\n",
      " deep -> [('balai', 0.8027281761169434), ('further', 0.7456870675086975), ('banai', 0.7449545860290527), ('token', 0.7228017449378967)]\n",
      " python -> [('november', 0.8383015990257263), ('script', 0.815959095954895), ('theme', 0.8116558194160461), ('the', 0.8040140271186829)]\n",
      " learning -> [('clockwork', 0.8150789141654968), ('bullets', 0.8136433959007263), ('attends', 0.8063232898712158), ('loftus', 0.801354169845581)]\n",
      "\n",
      "Sample CBOW predictions (context -> predicted target):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 184\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_loader_cbow))):\n\u001b[0;32m--> 184\u001b[0m         contexts, tgt \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loader_cbow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    185\u001b[0m         logp \u001b[38;5;241m=\u001b[39m cbow_model(contexts\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    186\u001b[0m         pred_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, contexts):\n",
    "        # contexts: (batch, context_len)\n",
    "        e = self.emb(contexts)         # (batch, context_len, emb)\n",
    "        e_mean = e.mean(dim=1)         # (batch, emb)\n",
    "        out = self.linear(e_mean)      # (batch, vocab)\n",
    "        return self.log_softmax(out)\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, targets):\n",
    "        # targets: (batch,)\n",
    "        e = self.emb(targets)          # (batch, emb)\n",
    "        out = self.linear(e)           # (batch, vocab)\n",
    "        return self.log_softmax(out)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Training and evaluation functions\n",
    "# ---------------------------\n",
    "def train_cbow(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for contexts, targets in loader:\n",
    "        contexts = contexts.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.long).to(device) if not isinstance(targets, torch.Tensor) else targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logp = model(contexts)\n",
    "        loss = criterion(logp, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * contexts.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_cbow(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device)\n",
    "            targets = torch.tensor(targets, dtype=torch.long).to(device) if not isinstance(targets, torch.Tensor) else targets.to(device)\n",
    "            logp = model(contexts)\n",
    "            loss = criterion(logp, targets)\n",
    "            total_loss += loss.item() * contexts.size(0)\n",
    "            preds = torch.argmax(logp, dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return total_loss / max(1, len(loader.dataset)), correct / max(1, total)\n",
    "\n",
    "def train_skip(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for targets, contexts in loader:\n",
    "        targets = torch.tensor(targets, dtype=torch.long).to(device) if not isinstance(targets, torch.Tensor) else targets.to(device)\n",
    "        contexts = torch.tensor(contexts, dtype=torch.long).to(device) if not isinstance(contexts, torch.Tensor) else contexts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logp = model(targets)\n",
    "        loss = criterion(logp, contexts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_skip(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    with torch.no_grad():\n",
    "        for targets, contexts in loader:\n",
    "            targets = torch.tensor(targets, dtype=torch.long).to(device) if not isinstance(targets, torch.Tensor) else targets.to(device)\n",
    "            contexts = torch.tensor(contexts, dtype=torch.long).to(device) if not isinstance(contexts, torch.Tensor) else contexts.to(device)\n",
    "            logp = model(targets)\n",
    "            loss = criterion(logp, contexts)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            preds = torch.argmax(logp, dim=1)\n",
    "            correct += (preds == contexts).sum().item()\n",
    "            total += contexts.size(0)\n",
    "    return total_loss / max(1, len(loader.dataset)), correct / max(1, total)\n",
    "\n",
    "# ---------------------------\n",
    "# Instantiate models & train\n",
    "# ---------------------------\n",
    "def Vocab_size(texts):\n",
    "    cleaned_text = []\n",
    "    words_list = []\n",
    "    pattern = re.compile(r'[^A-Za-z0-9\\s]')\n",
    "    for text in texts:\n",
    "        if text !='':\n",
    "            clean_text = pattern.sub('', text).lower()\n",
    "            words = [word for word in clean_text.split() if word]\n",
    "            words_list.extend(words)\n",
    "            cleaned_text.append(words)\n",
    "    word_counts = {}\n",
    "    for text in words_list:\n",
    "        if text in word_counts.keys():\n",
    "            word_counts[text] = int(word_counts[text]) + 1            \n",
    "        else :\n",
    "            word_counts[text] = 1\n",
    "    list_word = [w for w,c in word_counts.items() if c>=2]\n",
    "    list_word = list_word + [\"<unk>\"]\n",
    "    word2idx = {word:idx for idx,word in enumerate(list_word)}\n",
    "    idx2word = {idx:word for word,idx in word2idx.items()}\n",
    "    return len(idx2word) , word2idx , idx2word\n",
    "\n",
    "vocab_size,word2idx,idx2word = Vocab_size(train)\n",
    "print(vocab_size)\n",
    "emb_dim = 20\n",
    "\n",
    "cbow_model = CBOWModel(vocab_size, emb_dim).to(device)\n",
    "skip_model = SkipGramModel(vocab_size, emb_dim).to(device)\n",
    "\n",
    "cbow_opt = torch.optim.Adam(cbow_model.parameters(), lr=0.01)\n",
    "skip_opt = torch.optim.Adam(skip_model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(\"Training CBOW...\")\n",
    "for ep in range(1, epochs + 1):\n",
    "    train_loss = train_cbow(cbow_model, train_loader_cbow, cbow_opt, criterion, device)\n",
    "    val_loss, val_acc = eval_cbow(cbow_model, valid_loader_cbow, device)\n",
    "    print(f\"CBOW epoch {ep:02d} train_loss={train_loss:.4f} test_loss={val_loss:.4f} test_acc={val_acc:.4f}\")\n",
    "\n",
    "print(\"Training Skip-gram...\")\n",
    "for ep in range(1, epochs + 1):\n",
    "    train_loss = train_skip(skip_model, train_loader_skipgram, skip_opt, criterion, device)\n",
    "    val_loss, val_acc = eval_skip(skip_model, valid_loader_skipgram, device)\n",
    "    print(f\"Skip epoch {ep:02d} train_loss={train_loss:.4f} test_loss={val_loss:.4f} test_acc={val_acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Inspect embeddings: nearest neighbors (cosine)\n",
    "# ---------------------------\n",
    "import math\n",
    "def nearest_neighbors(model_emb, word_idx, idx2word, topk=5):\n",
    "    emb = model_emb.weight.data.cpu()\n",
    "    query = emb[word_idx].unsqueeze(0)\n",
    "    norms = emb.norm(dim=1, keepdim=True)\n",
    "    qnorm = query.norm()\n",
    "    sims = (emb @ query.t()).squeeze(1) / (norms.squeeze(1) * qnorm + 1e-9)\n",
    "    vals, inds = sims.topk(min(topk + 1, len(inds:=range(len(idx2word)))))\n",
    "    neighbors = []\n",
    "    for v, i in zip(vals.tolist(), inds.tolist()):\n",
    "        neighbors.append((idx2word[i], v))\n",
    "    neighbors = [n for n in neighbors if n[0] != idx2word[word_idx]][:topk]\n",
    "    return neighbors\n",
    "\n",
    "print(\"\\nExample neighbors using CBOW embeddings:\")\n",
    "for w in [\"i\", \"deep\", \"python\", \"learning\"]:\n",
    "    idx = word2idx.get(w, 1)\n",
    "    print(f\" {w} ->\", nearest_neighbors(cbow_model.emb, idx, idx2word, topk=4))\n",
    "\n",
    "print(\"\\nExample neighbors using Skip-gram embeddings:\")\n",
    "for w in [\"i\", \"deep\", \"python\", \"learning\"]:\n",
    "    idx = word2idx.get(w, 1)\n",
    "    print(f\" {w} ->\", nearest_neighbors(skip_model.emb, idx, idx2word, topk=4))\n",
    "\n",
    "# ---------------------------\n",
    "# Print sample predictions\n",
    "# ---------------------------\n",
    "print(\"\\nSample CBOW predictions (context -> predicted target):\")\n",
    "cbow_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(min(6, len(test_loader_cbow))):\n",
    "        contexts, tgt = test_loader_cbow[i]\n",
    "        logp = cbow_model(contexts.unsqueeze(0).to(device))\n",
    "        pred_idx = torch.argmax(logp, dim=1).item()\n",
    "        ctx_words = [idx2word[c.item()] for c in contexts]\n",
    "        print(f\" context={ctx_words} -> pred={idx2word[pred_idx]} (truth={idx2word[tgt]})\")\n",
    "\n",
    "print(\"\\nSample Skip-gram predictions (target -> predicted context):\")\n",
    "skip_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(min(8, len(test_loader_skipgram))):\n",
    "        tgt, ctx = test_loader_skipgram[i]\n",
    "        logp = skip_model(torch.tensor([tgt], dtype=torch.long).to(device))\n",
    "        pred_idx = torch.argmax(logp, dim=1).item()\n",
    "        print(f\" target={idx2word[tgt]} -> pred={idx2word[pred_idx]} (truth={idx2word[ctx]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ููุงุณู ุดุจฺฉูโูุง</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding: 15px; background-color: #f5f5f5; border-radius: 12px; border: 2px solid #022216; font-family: Vazir; line-height: 1.8; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "<h3 style=\"color: #022216; margin-top: 0;\">ุฏุณุชูุฑุงูุนูู ุชุญูู ุดุจุงูุช ูุงฺฺฏุงู</h3>\n",
    "\n",
    "<ol style=\"padding-right: 20px;\">\n",
    "    <li><strong>ุงูุชุฎุงุจ ูุงฺูโูุง:</strong> ต ูุงฺู ุจู ุฏูุฎูุงู ุงูุชุฎุงุจ ููุงุฏ.</li>\n",
    "    <li><strong>ุงูุชู ูุงฺฺฏุงู ูุดุงุจู:</strong> ุจุฑุง ูุฑ ูุงฺู ู ูุฑ ูุฏูุ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุนุงุฑ <em>Cosine Similarity</em>ุ ต ูุงฺู ุจุฑุชุฑ ูุดุงุจู ุฑุง ุงุณุชุฎุฑุงุฌ ฺฉูุฏ.</li>\n",
    "    <li><strong>ููุงุด ุจุตุฑ:</strong> ูุงฺฺฏุงู ูุดุงุจู ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุฑูุด <em>t-SNE</em> ุจู ุตูุฑุช ูููุฏุงุฑ ููุงุด ุฏูุฏ (ุจุฑุง ูุฑ ูุงฺู ู ูุฑ ูุฏู ฺฉ ูููุฏุงุฑ ุฌุฏุงฺฏุงูู).</li>\n",
    "    <li><strong>ุชุญูู ููุงุณูโุง:</strong> ูููุฏุงุฑูุง ุชููุฏ ุดุฏู ุฑุง ุจุฑุง ุฏู ูุฏู ูุฎุชูู ุจุง ฺฉุฏฺฏุฑ ููุงุณู ฺฉูุฏ.</li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"color: #4b5563; font-size: 0.9em; margin-bottom: 0;\">\n",
    "    ูฺฉุชู: ุฏุฑ ูุฑ ูููุฏุงุฑ t-SNE ูโุจุงุณุช ูุงฺู ุงุตู ุจู ููุฑุงู ต ูุงฺู ูุดุงุจู ุขู ููุงุด ุฏุงุฏู ุดูุฏ.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ:</b><br>\n",
    "{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุฏูู: ูพุงุฏูโุณุงุฒ ุทุจููโุจูุฏ ุงุฎุจุงุฑ ุจุง ฺฉูฺฉ ุดุจฺฉู ุนุตุจ ู ูุฏู Fasttext<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงู ุดูุง ุจุง ฺฉูฺฉ ุดุจฺฉู ุนุตุจ ุชูุงู ูุชุตู \n",
    "(Fully Connected)\n",
    "ฺฉ ุทุจููโุจูุฏ ูุชู ูพุงุฏูโุณุงุฒ ุฎูุงูุฏ\n",
    "ฺฉุฑุฏ.\n",
    "ููฺูู ุงุฒ ูุฏู\n",
    "<a href=\"https://fasttext.cc/\">Fasttext</a>\n",
    "ุจุฑุง Embed\n",
    "ฺฉุฑุฏู ูุชูโูุง ุงุณุชูุงุฏู ูโฺฉูุฏ.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏู ู ูพุด ูพุฑุฏุงุฒุด</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "    <ul>\n",
    "        <li>ุงุจุชุฏุง ุฏุชุงุณุช ุฒุฑ ุฏุงูููุฏ ฺฉูุฏ.\n",
    "    <br>\n",
    "    <a href=\"https://huggingface.co/datasets/SetFit/ag_news\">link</a></li>\n",
    "        <li>\n",
    "            ูพุณ ุงุฒ ุฏุงูููุฏ ูุฌููุนูโุฏุงุฏูุ 5000\n",
    "            ููููู ุงุฒ ูุฌููุนูโุฏุงุฏู ุขููุฒุด ู 2000 ููููู ุงุฒ ูุฌููุนูโุฏุงุฏู ุชุณุช ุฑุง ุจู ุชุตุงุฏู ุงูุชุฎุงุจ ฺฉูุฏ.\n",
    "        </li>\n",
    "        <li>\n",
    "            ูุฌููุนู 2000 ูููููโุง ุฑุง ุจู ุฏู ูุฌููุนู 1000 ุชุง ุชุณุช ู ุงุฑุฒุงุจ ุชูุณู ฺฉูุฏ. ููฺูู ุงุฒ ูุฌููุนู 5000 ุชุง ุจู ุนููุงู ูุฌููุนูโุฏุงุฏู ุขููุฒุด ุงุณุชูุงุฏู ฺฉูุฏ.\n",
    "        </li>\n",
    "        <li>\n",
    "            ุฏุฑ ููฺฏุงู ุชุดฺฉู ูุฌููุนูโุฏุงุฏูโูุง ุฌุฏุฏ ุญุชูุง ุชูุฌู ุฏุงุดุชูโุจุงุดุฏ ฺฉู ุชูุฒุน ุฏุงุฏูโูุง ุฏุฑ ูุฑ ฺฉูุงุณ balanced ุจุงุดุฏ.\n",
    "        </li>\n",
    "        <li>\n",
    "        ุจุฑุง ูพุดโูพุฑุฏุงุฒุด ูุชูู ุชููุง ุงุณุชูุงุฏู ุงุฒ lowercasing ู \n",
    "            ุญุฐู white space ูุง ุงุถุงูู \n",
    "            ฺฉุงู ุงุณุช.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <b>ุณูุงู:</b> ุฏุฑ ุงู ุจุฎุด ุจู ุฏูู ุงุณุชูุงุฏู ุงุฒ ูุฏู fasttext\n",
    "    ุจุฑุง embed ฺฉุฑุฏู \n",
    "        ูุชูู ูุงุฒ ุจู ูพุดโูพุฑุฏุงุฒุด ุฒุงุฏ ูุฏุงุฑู. ฺู ูฺฺฏ ุงู ูุฏู ุจุงุนุซ ูโุดูุฏ ฺฉู ูุง ุงุฒ ูพุด ูพุฑุฏุงุฒุด ุจุดุชุฑ ุจโูุงุฒ ุดููุ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ:</b><br>\n",
    "{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ูุฌููุนูโุฏุงุฏูโูุง Trainุ Test ู Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Embedding ูููููโูุง</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุจุฎุด ูโุฎูุงูู ูุชูู ุฑุง ุจุฑุง ุทุจููโุจูุฏ ุขูุงุฏู ฺฉูู.\n",
    "    <ol>\n",
    "        <li>\n",
    "            ุฏุฑ ุงุจุชุฏุง ูุฏู pre-train ุดุฏู \n",
    "            <a href=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.bin.zip\">wiki-news-300d-1M-subword</a>\n",
    "            ุฑุง ุฏุงูููุฏ ฺฉูุฏ ู ุจุง ฺฉูฺฉ ฺฉุชุงุจุฎูุงูู fasttext \n",
    "            ุขูุฑุง load ฺฉูุฏ.\n",
    "        </li>\n",
    "        <li>\n",
    "            ุฏุฑ ุงุณูุงุฏ ุดุดู ุฏุฑุณ ุจุง ููููู sentence embedding ุขุดูุง ุดุฏูโุงุฏ.\n",
    "            ุจุง ฺฉูฺฉ ูุฏู fasttext \n",
    "            ุชูุงู ูุชูู ููุฌูุฏ ุฏุฑ ูุฌููุนูโุฏุงุฏูโูุง trainุ test ู validation\n",
    "            ุฑุง embed ฺฉูุฏ.\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ุจุฑุฏุงุฑูุง Embedding\n",
    "ูุฌููุนูโุฏุงุฏูโูุง Train, Test, Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุงุฏู ุณุงุฒ ุดุจฺฉู ูโ ุงููุฒุด ุดุจฺฉู</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุญุงู ฺฉู ูุชูโูุง ุฑุง ุจู ูุถุง ุจุฑุฏุงุฑ ูฺฏุงุดุช ฺฉุฑุฏูโุงูุ ูุฌููุนูโุฏุงุฏูโูุง ุจุฑุง ุทุจููโุจูุฏ ุจุง ฺฉูฺฉ ุดุจฺฉู ุนุตุจ ุขูุงุฏู ูุณุชูุฏ.\n",
    "<ol>\n",
    "    <li>ูุฌููุนูโุฏุงุฏู ูุง ุฑุง ุจุง ฺฉูฺฉ Dataloader pytorch\n",
    "    ุง ูุฑ framework ุฏฺฏุฑ ฺฉู ุงุณุชูุงุฏู ูโฺฉูุจุฏ ุขูุงุฏู ฺฉูุฏ.\n",
    "    </li>\n",
    "    <li>\n",
    "    ฺฉ ุดุจฺฉู ุนุตุจ Fully Connected ุจุง ูุนูุงุฑ ุฏูุฎูุงู ุจุฑุง ุขููุฒุด ุทุฑุงุญ ฺฉูุฏ.\n",
    "    </li>\n",
    "    <li>\n",
    "    ุดุจฺฉู ุฑุง ุญุฏุงูู ุจู ุงูุฏุงุฒู 30 ุงูพุงฺฉ ุขููุฒุด ุฏูุฏ.\n",
    "    </li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    ูุฏู ุขููุฒุด ุฏุงุฏู ุดุฏู\n",
    "    </li>\n",
    "    <li>\n",
    "    ูููุฏุงุฑ ุชุบุฑุงุช ุฏูุช ู loss\n",
    "        ุฏุฑ ููฺฏุงู ุขููุฒุด ุจุฑุฑู ุฏู ูุฌููุนูโุฏุงุฏู\n",
    "        Train ู Validation\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุชุญูู ูุชุงุฌ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูพุณ ุงุฒ ุงุชูุงู ุขููุฒุดุ ููุงุฑุฏ ุฒุฑ ุฑุง ุจุฑุง ูุฌููุนู ุฏุงุฏู test ุจุฏุณุช ุขูุฑุฏ:\n",
    "    <ul>\n",
    "        <li>\n",
    "            ูุงุชุฑุณ ุฏุฑููโุฑุฎุชฺฏ\n",
    "            (Confusion Matrix)\n",
    "        </li>\n",
    "        <li>\n",
    "            ุฏูุช\n",
    "        </li>\n",
    "        <li>\n",
    "            Macro-F1\n",
    "        </li>\n",
    "    </ul>\n",
    "    ุจุง ุชูุฌู ุจู ูุชุฑฺฉโูุงุ ุจู ุณูุงูุงุช ุฒุฑ ุฌูุงุจ ุฏูุฏ:\n",
    "    <ul>\n",
    "        <li>ูุฏู ุฏุฑ ุชุดุฎุต ฺฉุฏุงู ฺฉูุงุณโูุง ุจูุชุฑู ู ุจุฏุชุฑู ุนููฺฉุฑุฏ ุฑุง ุฏุงุดุชูโุงุณุชุ</li>\n",
    "        <li>ูุฏู ฺฉุฏุงู ฺฉูุงุณโูุง ุฑุง ุจุดุชุฑู ุฏูุนู ุจุง ฺฉุฏฺฏุฑ ุงุดุชุจุงู ฺฏุฑูุชูโุงุณุชุ</li>\n",
    "        <li>ุณู ููููู ุงุฒ ูุชูู ฺฉู ูุฏู ุจู ุงุดุชุจุงู ุขููุง ุฑุง ุจุฑฺุณุจ ุฒุฏูโุงุณุช ูพุฏุง ฺฉูุฏ ู ูุญุชูุง ุขููุง ุฑุง ุจุฑุฑุณ ฺฉูุฏ.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ:</b><br>\n",
    "{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    ูุชุฑฺฉโูุง ฺฏูุชูโุดุฏู\n",
    "    </li>\n",
    "    <li>\n",
    "    ุชุญูู ูุชุงุฌ\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**ูฺฉุงุช ููู ู ููุงูู ุชุญูู**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุงู ุงุฑุณุงู ุดูุง ุจุงุฏ ุจุง ูุฑูุช ุฒุฑ ูุงูฺฏุฐุงุฑ ุดูุฏ: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุญูู ุงูุฌุงู ุชูุฑู:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>ุณูููโูุง ฺฉุฏ ุจุง ุจุฑฺุณุจ <code>WRITE YOUR CODE HERE</code> ุฑุง ุชฺฉูู ฺฉูุฏ.</li>\n",
    "  <li>ุจุฑุง ูพุงุณุฎโูุง ูุชูุ ูุชู <code>{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}</code> ุฑุง ุจุง ูพุงุณุฎ ุฎูุฏ ุฌุงฺฏุฒู ฺฉูุฏ.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุตุฏุงูุช ุนูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ูุง ููุชโุจูฺฉโูุง ุชุนุฏุงุฏ ูุดุฎุต ุงุฒ ุฏุงูุดุฌูุงู ฺฉู ุจู ุตูุฑุช ุชุตุงุฏู ุงูุชุฎุงุจ ูโุดููุฏุ ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ุงู ุจุฑุฑุณโูุง ุงุทููุงู ุญุงุตู ูโฺฉููุฏ ฺฉู ฺฉุฏ ฺฉู ููุดุชุฏ ูุงูุนุงู ูพุงุณุฎโูุง ููุฌูุฏ ุฏุฑ ููุชโุจูฺฉ ุดูุง ุฑุง ุชููุฏ ูโฺฉูุฏ. ุงฺฏุฑ ูพุงุณุฎโูุง ุตุญุญ ุฑุง ุฏุฑ ููุชโุจูฺฉ ุฎูุฏ ุจุฏูู ฺฉุฏ ฺฉู ูุงูุนุงู ุขู ูพุงุณุฎโูุง ุฑุง ุชููุฏ ฺฉูุฏ ุชุญูู ุฏูุฏุ ุงู ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> <li>ูุง ููฺูู ุจุฑุฑุณโูุง ุฎูุฏฺฉุงุฑ ุฑุง ุจุฑุง ุชุดุฎุต ุณุฑูุช ุนูู ุฏุฑ ููุชโุจูฺฉโูุง ฺฉููุจ ุงูุฌุงู ุฎูุงูู ุฏุงุฏ. ฺฉูพ ฺฉุฑุฏู ฺฉุฏ ุงุฒ ุฏฺฏุฑุงู ูุฒ ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุชูุถุญุงุช ุชฺฉูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "ุฎูุงูุง ู ุฏูุช ุจุฑุฑุณโูุง ุฏุฑ ฺฏุฒุงุฑุด ููุง ุงุฒ ุงููุช ูฺูโุง ุจุฑุฎูุฑุฏุงุฑ ุงุณุช. ุจู ุชูุฑูโูุง ฺฉู ุจู ุตูุฑุช ฺฉุงุบุฐ ุชุญูู ุฏุงุฏู ุดููุฏ ุง ุจู ุตูุฑุช ุนฺฉุณ ุฏุฑ ุณุงุช ุจุงุฑฺฏุฐุงุฑ ุดููุฏุ ุชุฑุชุจ ุงุซุฑ ุฏุงุฏู ูุฎูุงูุฏ ุดุฏ.</li>\n",
    "<li>\n",
    " ูููโ ฺฉุฏูุง ูพูุณุช ฺฏุฒุงุฑุด ุจุงุณุช ูุงุจูุช ุงุฌุฑุง ูุฌุฏุฏ ุฏุงุดุชู ุจุงุดูุฏ. ุฏุฑ ุตูุฑุช ฺฉู ุจุฑุง ุงุฌุฑุง ูุฌุฏุฏ ุขูโูุง ูุงุฒ ุจู ุชูุธูุงุช ุฎุงุต ูโุจุงุดุฏุ ุจุงุณุช ุชูุธูุงุช ููุฑุฏ ูุงุฒ ุฑุง ูุฒ ุฏุฑ ฺฏุฒุงุฑุด ุฎูุฏ ุฐฺฉุฑ ฺฉูุฏ.  ุฏูุช ฺฉูุฏ ฺฉู  ุชูุงู ฺฉุฏูุง ุจุงุฏ ุชูุณุท ุดูุง ุงุฌุฑุง ุดุฏู ุจุงุดูุฏ ู ูุชุงุฌ ุงุฌุฑุง ุฏุฑ ูุงู ฺฉุฏูุง ุงุฑุณุงู ูุดุฎุต ุจุงุดุฏ. ุจู ฺฉุฏูุง ฺฉู ูุชุงุฌ ุงุฌุฑุง ุขูโูุง ุฏุฑ ูุงู ุงุฑุณุงู ูุดุฎุต ูุจุงุดุฏ ููุฑูโุง ุชุนูู ููโฺฏุฑุฏ.\n",
    "</li>\n",
    "<li>ุชูุฌู ฺฉูุฏ ุงู ุชูุฑู ุจุงุฏ ุจู ุตูุฑุช ุชฺฉโููุฑู ุงูุฌุงู ุดูุฏ ู ูพุงุณุฎโูุง ุงุฑุงุฆู ุดุฏู ุจุงุฏ ูุชุฌู ูุนุงูุช ูุฑุฏ ููุณูุฏู ุจุงุดุฏ (ูููฺฉุฑ ู ุจู ุงุชูุงู ูู ููุดุชู ุชูุฑู ูุฒ ููููุน ุงุณุช). ุฏุฑ ุตูุฑุช ูุดุงูุฏู\n",
    " ุชุดุงุจู ุจู ููู ุงูุฑุงุฏ ูุดุงุฑฺฉุชโฺฉููุฏูุ ููุฑู ุชูุฑู ุตูุฑ ู ุจู ุงุณุชุงุฏ ฺฏุฒุงุฑุด ูโฺฏุฑุฏุฏ.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "ูุทูุงู ุชูุงู ูพุงุณุฎโูุง ูุชู ุฎูุฏ ุฑุง ุจุง <b>ูููุช ูุฒุฑ (Vazir)</b> ู ุจูโุตูุฑุช <b>ุฑุงุณุชโฺู</b> ุจููุณุฏ.  \n",
    "ุงุฒ ุงุณุชูุงุฏู ุงุฒ ูููุชโูุง ูพุดโูุฑุถ ุฎูุฏุฏุงุฑ ฺฉูุฏ ุชุง ุธุงูุฑ ููุชโุจูฺฉ ุดูุง ฺฉโุฏุณุช ู ุฎูุงูุง ุจุงุดุฏ.  \n",
    "ุฏุฑ ุจุฎุดโูุง ุชุดุฑุญุ ุณุน ฺฉูุฏ ูพุงุณุฎโูุง ุฑุง ฺฉุงููุ ููุณุฌู ู ุจุง ุฑุนุงุช ูฺฏุงุฑุด ูุงุฑุณ ุจููุณุฏ.  \n",
    "ููฺููุ ุจู ฺูุด ุชูุฒ ุณูููโูุง ู ุงุฌุฑุง ุฏุฑุณุช ฺฉุฏูุง ุชูุฌู ฺฉูุฏ ุชุง ุชูุฑู ุดูุง ุจุง ูุฑูุช ุฎูุงุณุชูโุดุฏู ู ุงุณุชุงูุฏุงุฑุฏ ุงุฑุงุฆู ุดูุฏ.\n",
    "</li>\n",
    " <li>ุจุฑุง ูุทุงูุนู ุจุดุชุฑ ุฏุฑุจุงุฑูโ ูุฑูุช Markdown ูโุชูุงูุฏ ุงุฒ <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">ุงู ููฺฉ</a> ูุทุงูุนู ฺฉูุฏ.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1301.3781\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "npl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
